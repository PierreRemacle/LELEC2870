{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project: Predicting a film’s gross revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import trustworthiness\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import  mutual_info_regression, SelectKBest\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "X1 = pd.read_csv(\"X1.csv\" )\n",
    "Y1 = pd.read_csv(\"Y1.csv\" , header=None , names =[\"revenue\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreProcessing(dataframe):\n",
    "\n",
    "    dataframe.insert(10, \"genre3\", \"\")\n",
    "    dataframe.insert(10, \"genre2\", \"\")\n",
    "    dataframe.insert(10, \"genre1\", \"\")\n",
    "\n",
    "    genre_individual(dataframe)\n",
    "    \n",
    "    toDrop = [\"Unnamed: 0\", \"title\", \"img_url\", \"description\", \"genres\", \"is_adult\"]\n",
    "    for col in toDrop :\n",
    "        dataframe=dataframe.drop(col,axis=1)\n",
    "\n",
    "    dataframe = fillRuntime(dataframe)  # fill empty runtime with NaN values\n",
    "    dataframe = weightedGenres(dataframe) # weighted on-hot encoding of genres\n",
    "    dataframe = oneHotStudio(dataframe)\n",
    "    dataframe = processEmbeddings(dataframe)\n",
    "    return dataframe\n",
    "    #    directorAndActors(dataframe)\n",
    "\n",
    "def fillRuntime(dataframe):\n",
    "\n",
    "    averageTimeByGenre = {}\n",
    "    for i in range (len (dataframe[\"genre1\"].unique())):\n",
    "        averageTimeByGenre[dataframe[\"genre1\"].unique()[i]] = 0\n",
    "    \n",
    "    for i in range (len (dataframe[\"runtime\"])):\n",
    "        if (dataframe[\"runtime\"][i] != '\\\\N'):\n",
    "            averageTimeByGenre[dataframe[\"genre1\"][i]]+= int(dataframe[\"runtime\"][i])            \n",
    "            \n",
    "    for i in range (len (dataframe[\"genre1\"].unique())):\n",
    "        averageTimeByGenre[dataframe[\"genre1\"].unique()[i]] /= dataframe[\"genre1\"].value_counts()[dataframe[\"genre1\"].unique()[i]]     \n",
    "\n",
    "    for i in range (len (dataframe[\"genre1\"])):\n",
    "        if dataframe[\"runtime\"][i] == '\\\\N':\n",
    "            dataframe[\"runtime\"][i] = int(averageTimeByGenre[dataframe[\"genre1\"][i]])\n",
    "        else: \n",
    "            dataframe[\"runtime\"][i] = int(dataframe[\"runtime\"][i])\n",
    "    return dataframe\n",
    "\n",
    "def weightedGenres(dataframe):\n",
    "\n",
    "    ListOfGenre = list(set(list(dataframe[\"genre1\"].unique()) + list(dataframe[\"genre2\"].unique()) + list(dataframe[\"genre3\"].unique())))\n",
    "    ListOfGenre.remove(\"\")\n",
    "    ListOfGenre.remove('\\\\N')\n",
    "    \n",
    "    for i in range (len(ListOfGenre)):\n",
    "        dataframe[ListOfGenre[i]] = 0\n",
    "\n",
    "    for i in range (len(dataframe[\"genre1\"])):\n",
    "        if dataframe[\"genre1\"][i] != \"\\\\N\":\n",
    "            dataframe[dataframe[\"genre1\"][i]][i] = 3\n",
    "            \n",
    "    for i in range (len(dataframe[\"genre2\"])):\n",
    "        if dataframe[\"genre2\"][i] != \"\\\\N\" and dataframe[\"genre2\"][i] != \"\":\n",
    "            dataframe[dataframe[\"genre2\"][i]][i] = 2\n",
    "            \n",
    "    for i in range (len(dataframe[\"genre3\"])):\n",
    "        if dataframe[\"genre3\"][i] != \"\\\\N\" and dataframe[\"genre3\"][i] != \"\":\n",
    "            dataframe[dataframe[\"genre3\"][i]][i] = 1\n",
    "            \n",
    "    dataframe=dataframe.drop(\"genre1\",axis=1)\n",
    "    dataframe=dataframe.drop(\"genre2\",axis=1)\n",
    "    dataframe=dataframe.drop(\"genre3\",axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def oneHotStudio(dataframe):\n",
    "\n",
    "    # Count the number of occurrences of each category\n",
    "    counts = dataframe[\"studio\"].value_counts()\n",
    "\n",
    "    # Sort the categories by their frequency in descending order\n",
    "    counts = counts.sort_values(ascending=False)\n",
    "    k = 9\n",
    "    # Select the k most represented categories\n",
    "    top_10 = counts[:k].index\n",
    "\n",
    "    # Apply binary encoding to the selected categories\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['studio'], prefix='std', prefix_sep='_', dummy_na=False)\n",
    "    for col in dataframe.columns:\n",
    "        if col[0:3] == \"std\":\n",
    "            if(col[4:] not in top_10):\n",
    "                dataframe.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    return text[text.startswith(prefix) and len(prefix):]\n",
    "\n",
    "def genre_individual(dataset):\n",
    "    for i in range (len(dataset[\"genres\"])):\n",
    "        genres_here = dataset[\"genres\"][i].split(\",\")\n",
    "        for j in range(len(genres_here)):\n",
    "            name = \"genre\"+str(j+1)\n",
    "            dataset.loc[i, name]=genres_here[j]\n",
    "\n",
    "def processEmbeddings(dataframe):\n",
    "\n",
    "    for i in range (len(dataframe)):\n",
    "        LaList = dataframe['text_embeddings'][i][1:][:-1].split(',')\n",
    "        dataframe['text_embeddings'][i] = [float(item) for item in LaList] \n",
    "\n",
    "    for i in range (len(dataframe)):\n",
    "        LaList = dataframe['img_embeddings'][i][1:][:-1].split(',')\n",
    "        dataframe['img_embeddings'][i] = [float(item) for item in LaList] \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def directorAndActors(dataset):\n",
    "    actorsList = []\n",
    "    directorList = []\n",
    "    for i in range (len(dataset[\"description\"])):\n",
    "        print(\"\\r\"+str(i), end= \"\")\n",
    "        step1= remove_prefix(dataset[\"description\"][i], dataset[\"title\"][i] + \": \")\n",
    "        step2= step1.split('. ')\n",
    "        if (\"Directed \" in step2[0]):\n",
    "            step3= remove_prefix(step2[0], \"Directed by \")\n",
    "            if (step3 not in directorList):\n",
    "                directorList.append(step3)\n",
    "                dataset[step3] = 0\n",
    "            dataset.at[i, step3] = 1\n",
    "            \n",
    "        elif (\"With\" in step2[0]):\n",
    "            step4= remove_prefix(step2[0], \"With \").split(', ')\n",
    "        if (len(step2) >=2):\n",
    "            if (\"With\" in step2[1]):\n",
    "                step4= remove_prefix(step2[1], \"With \").split(', ')\n",
    "            \n",
    "        \n",
    "        for j in range (len(step4)):\n",
    "            if step4[j] not in actorsList:\n",
    "                actorsList.append(step4[j])\n",
    "                dataset[step4[j]] = 0\n",
    "                dataset.at[i, step4[j]] = 1\n",
    "            else :\n",
    "                dataset.at[i, step4[j]] = 1\n",
    "    return actorsList\n",
    "\n",
    "def color_Dico():\n",
    "    genres = set()\n",
    "\n",
    "    for i in range (len(X1[\"genres\"])):\n",
    "        for j in range (len(X1[\"genres\"][i].split(\",\"))):\n",
    "            genres.add(X1[\"genres\"][i].split(\",\")[j])\n",
    "    \n",
    "    rgb_values = sns.color_palette(\"Set2\", 27)\n",
    "    dico = {}\n",
    "    for i in range (len(genres)):\n",
    "        dico[genres.pop()]=rgb_values[i]\n",
    "    return dico\n",
    "\n",
    "X1 = DataPreProcessing(X1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionnality reduction of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToNpArray(pdSeries):\n",
    "    \n",
    "    list = pdSeries.tolist()\n",
    "    npArray = np.array(list) \n",
    "    return npArray\n",
    "\n",
    "\n",
    "data = ToNpArray(X1['text_embeddings'])\n",
    "data = StandardScaler().fit_transform(data)  # scaling the data\n",
    "\n",
    "def compute_embeddings():\n",
    "    textembed3 =  TSNE(n_components=3, random_state=42, method='exact').fit_transform(data)\n",
    "    np.save('embed/text3.npy', textembed3)\n",
    "\n",
    "    data = ToNpArray(X1['img_embeddings'])\n",
    "    data = StandardScaler().fit_transform(data)  # scaling the data\n",
    "    imgembed3 =  TSNE(n_components=3, random_state=42, method='exact').fit_transform(data)\n",
    "    np.save('embed/imgembed3.npy', imgembed3)\n",
    "    imgembed6 =  TSNE(n_components=6, random_state=42, method='exact').fit_transform(data)\n",
    "    np.save('embed/imgembed6.npy', imgembed6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textembed = np.load(\"embed/text3.npy\")\n",
    "X1.drop(\"text_embeddings\", axis=1, inplace=True)\n",
    "for i in range(3):\n",
    "    X1[\"text_embeddings%d\"%(i)] = textembed[:,i]\n",
    "\n",
    "\n",
    "imgembed = np.load(\"embed/imgembed3.npy\")\n",
    "X1.drop(\"img_embeddings\", axis=1, inplace= True)\n",
    "for i in range(3):\n",
    "    X1[\"img_embeddings%d\"%(i)] = imgembed[:,i]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1_scaled = scaler.fit_transform(X1.to_numpy())\n",
    "X1= pd.DataFrame(X1_scaled, columns=X1.columns)\n",
    "\n",
    "X1.to_csv(\"processedData/st9im3txt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection & extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>production_year</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_year</th>\n",
       "      <th>Short</th>\n",
       "      <th>Music</th>\n",
       "      <th>News</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>std_Par.</th>\n",
       "      <th>std_Sony</th>\n",
       "      <th>std_Uni.</th>\n",
       "      <th>std_WB</th>\n",
       "      <th>text_embeddings0</th>\n",
       "      <th>text_embeddings1</th>\n",
       "      <th>text_embeddings2</th>\n",
       "      <th>img_embeddings0</th>\n",
       "      <th>img_embeddings1</th>\n",
       "      <th>img_embeddings2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062975</td>\n",
       "      <td>0.809200</td>\n",
       "      <td>1.111354</td>\n",
       "      <td>0.430581</td>\n",
       "      <td>0.933839</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.219846</td>\n",
       "      <td>-0.345771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193687</td>\n",
       "      <td>-0.173113</td>\n",
       "      <td>-0.211083</td>\n",
       "      <td>-0.234152</td>\n",
       "      <td>-0.456871</td>\n",
       "      <td>0.145407</td>\n",
       "      <td>0.444517</td>\n",
       "      <td>-0.769388</td>\n",
       "      <td>-0.558291</td>\n",
       "      <td>1.482851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.263828</td>\n",
       "      <td>-0.271776</td>\n",
       "      <td>-0.073389</td>\n",
       "      <td>-0.252887</td>\n",
       "      <td>1.366959</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.219846</td>\n",
       "      <td>-0.345771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193687</td>\n",
       "      <td>-0.173113</td>\n",
       "      <td>-0.211083</td>\n",
       "      <td>-0.234152</td>\n",
       "      <td>-0.931708</td>\n",
       "      <td>-0.756918</td>\n",
       "      <td>1.154724</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>-1.202546</td>\n",
       "      <td>-1.876219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.451677</td>\n",
       "      <td>-0.256258</td>\n",
       "      <td>-1.596629</td>\n",
       "      <td>1.114049</td>\n",
       "      <td>-2.531122</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.219846</td>\n",
       "      <td>-0.345771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193687</td>\n",
       "      <td>-0.173113</td>\n",
       "      <td>-0.211083</td>\n",
       "      <td>-0.234152</td>\n",
       "      <td>0.334171</td>\n",
       "      <td>1.056132</td>\n",
       "      <td>-0.831411</td>\n",
       "      <td>1.581208</td>\n",
       "      <td>0.102715</td>\n",
       "      <td>0.564466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148750</td>\n",
       "      <td>-0.215474</td>\n",
       "      <td>-0.242638</td>\n",
       "      <td>0.430581</td>\n",
       "      <td>-0.798642</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.219846</td>\n",
       "      <td>-0.345771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193687</td>\n",
       "      <td>-0.173113</td>\n",
       "      <td>-0.211083</td>\n",
       "      <td>-0.234152</td>\n",
       "      <td>0.826204</td>\n",
       "      <td>-0.160741</td>\n",
       "      <td>0.399846</td>\n",
       "      <td>-0.729842</td>\n",
       "      <td>-0.143565</td>\n",
       "      <td>-1.128653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.995631</td>\n",
       "      <td>-0.265518</td>\n",
       "      <td>-1.258132</td>\n",
       "      <td>0.523781</td>\n",
       "      <td>-2.098002</td>\n",
       "      <td>-0.136678</td>\n",
       "      <td>-0.194455</td>\n",
       "      <td>-0.023776</td>\n",
       "      <td>-0.219846</td>\n",
       "      <td>-0.345771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193687</td>\n",
       "      <td>-0.173113</td>\n",
       "      <td>-0.211083</td>\n",
       "      <td>-0.234152</td>\n",
       "      <td>0.073563</td>\n",
       "      <td>-1.166933</td>\n",
       "      <td>0.823457</td>\n",
       "      <td>1.462627</td>\n",
       "      <td>-0.783300</td>\n",
       "      <td>0.206789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ratings   n_votes  production_year   runtime  release_year     Short  \\\n",
       "0  0.062975  0.809200         1.111354  0.430581      0.933839 -0.136678   \n",
       "1  1.263828 -0.271776        -0.073389 -0.252887      1.366959 -0.136678   \n",
       "2 -0.451677 -0.256258        -1.596629  1.114049     -2.531122 -0.136678   \n",
       "3  0.148750 -0.215474        -0.242638  0.430581     -0.798642 -0.136678   \n",
       "4 -1.995631 -0.265518        -1.258132  0.523781     -2.098002 -0.136678   \n",
       "\n",
       "      Music      News   Fantasy  Documentary  ...  std_Par.  std_Sony  \\\n",
       "0 -0.194455 -0.023776 -0.219846    -0.345771  ... -0.193687 -0.173113   \n",
       "1 -0.194455 -0.023776 -0.219846    -0.345771  ... -0.193687 -0.173113   \n",
       "2 -0.194455 -0.023776 -0.219846    -0.345771  ... -0.193687 -0.173113   \n",
       "3 -0.194455 -0.023776 -0.219846    -0.345771  ... -0.193687 -0.173113   \n",
       "4 -0.194455 -0.023776 -0.219846    -0.345771  ... -0.193687 -0.173113   \n",
       "\n",
       "   std_Uni.    std_WB  text_embeddings0  text_embeddings1  text_embeddings2  \\\n",
       "0 -0.211083 -0.234152         -0.456871          0.145407          0.444517   \n",
       "1 -0.211083 -0.234152         -0.931708         -0.756918          1.154724   \n",
       "2 -0.211083 -0.234152          0.334171          1.056132         -0.831411   \n",
       "3 -0.211083 -0.234152          0.826204         -0.160741          0.399846   \n",
       "4 -0.211083 -0.234152          0.073563         -1.166933          0.823457   \n",
       "\n",
       "   img_embeddings0  img_embeddings1  img_embeddings2  \n",
       "0        -0.769388        -0.558291         1.482851  \n",
       "1         0.072343        -1.202546        -1.876219  \n",
       "2         1.581208         0.102715         0.564466  \n",
       "3        -0.729842        -0.143565        -1.128653  \n",
       "4         1.462627        -0.783300         0.206789  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = pd.read_csv(\"processedData/st9im3txt3\", index_col=0)\n",
    "Y1 = pd.read_csv(\"Y1.csv\", header=None , names =[\"revenue\"])\n",
    "\n",
    "X1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we first do a PCA analysis\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "X1 = pca.fit_transform(X1)\n",
    "print(X1.shape)\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "print(exp_var_pca)\n",
    "\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "#\n",
    "# Create the visualization plot\n",
    "#\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_votes', 'release_year', 'production_year', 'runtime', 'ratings', 'Drama', 'text_embeddings2', 'text_embeddings1', 'text_embeddings0', 'Comedy', 'Adventure', 'Documentary', 'std_Uni.', 'Action', 'Crime']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 : feature selection using min redundancy max relevance approach\n",
    "\n",
    "# Compute the Pearson correlation between each feature and the target variable\n",
    "\n",
    "def minRedMaxRel(X1, Y1, k):\n",
    "\n",
    "    available = list(X1.columns)\n",
    "    selected = []\n",
    "    corrThreshold = 0.6\n",
    "    for i in range(k):\n",
    "\n",
    "        if(len(available) == 0):\n",
    "            break\n",
    "        mi = np.argsort(mutual_info_regression(X1[available].to_numpy(), Y1.to_numpy().ravel()))\n",
    "        best = available[mi[-1]]\n",
    "        selected.append(best)\n",
    "        for av in available:     \n",
    "            corrcoeff, _ = pearsonr(X1[av].to_numpy(), X1[best].to_numpy())\n",
    "            if corrcoeff > corrThreshold:\n",
    "                available.remove(av)\n",
    "\n",
    "    return X1[selected], selected\n",
    "    \n",
    "def Kbest(X1, Y1, k):\n",
    "\n",
    "    \"\"\"\"select k best based on mutual information, does not look at correlation \"\"\"\n",
    "    available = list(X1.columns)   \n",
    "    selected = []\n",
    "    mi = np.argsort(mutual_info_regression(X1[available].to_numpy(), Y1.to_numpy().ravel()))\n",
    "    for i in range(k):\n",
    "        best = available[mi[-i]]\n",
    "        selected.append(best)\n",
    "\n",
    "    return X1[selected], selected\n",
    "\n",
    "\n",
    "    \n",
    "X_selected1, selected1  = minRedMaxRel(X1, Y1, 15)\n",
    "print(selected1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction, we first project and then extract based on mutual information\n",
    "embeddings = [PCA, TSNE, Isomap, MDS]\n",
    "def feature_extraction(X1, Y1, d, k, embed):\n",
    "\n",
    "    \"\"\" performs dimensionnality reduction on the whole dataset before extracting most relevant features with respect to target variable\n",
    "        d : dimension after dimensionnality reduction\n",
    "        k : number of features extracted\n",
    "    \"\"\"\n",
    "    X1 = embed(n_components=d).fit_transform(X1)  \n",
    "    mi = np.argsort(mutual_info_regression(X1, Y1.to_numpy().ravel()))\n",
    "    extracted = np.array([X1[:, mi[-1]]])\n",
    "    for i in range(2, k+1):\n",
    "        extracted = np.concatenate((extracted, np.array([X1[:, mi[-i]]])), axis=0)\n",
    "\n",
    "    return extracted.T\n",
    "\n",
    "extracted = feature_extraction(X1, Y1, 10, PCA)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07611034788035911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Linear Regression model\n",
    "\n",
    "X = X_selected1.to_numpy()\n",
    "y = Y1.to_numpy()\n",
    "# Create a KFold object with 10 folds\n",
    "\n",
    "def Kfold(X, y, k, model):\n",
    "\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize a list to store the scores\n",
    "    scores = []\n",
    "\n",
    "    # Loop through the folds\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the test data and store the score\n",
    "        score = model.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "    # Calculate the mean score across all folds\n",
    "    mean_score = np.mean(scores)\n",
    "    return mean_score\n",
    "\n",
    "# Print the mean score\n",
    "#print(Kfold(X, y, 10, LinearRegression()))\n",
    "#print(Kfold(X, y, 10, KNeighborsRegressor(20)))\n",
    "#print(Kfold(X, y, 10, SVR(kernel='linear')))\n",
    "print(Kfold(X, y, 10, DecisionTreeRegressor()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac9f97180bfd2bf58793e78dc03ff0c56e685573b0d69d1ef096f6c360541cc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
